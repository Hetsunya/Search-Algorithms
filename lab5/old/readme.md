Давайте подробно разберём каждую функцию в последней версии кода, объясним их назначение, теоретическую основу и формулы, где они применяются. Это поможет вам лучше понять, как работает программа, и подготовиться к защите лабораторной работы.

---

### Теоретическая основа

1. **TF (Term Frequency)**:
   - **Определение**: Частота термина в документе — это мера, показывающая, насколько часто слово встречается в конкретном документе.
   - **Формула**:  
     \[
     TF(t, d) = \frac{\text{количество вхождений термина } t \text{ в документе } d}{\text{общее количество слов в документе } d}
     \]
   - **Смысл**: Нормализация по длине документа делает TF независимым от размера текста.

2. **IDF (Inverse Document Frequency)**:
   - **Определение**: Обратная документная частота измеряет редкость термина в корпусе документов. Чем реже слово встречается, тем выше его IDF.
   - **Формула** (в нашем коде с улучшением):  
     \[
     IDF(t, D) = \log\left(1 + \frac{|D|}{\text{df}(t) + 1}\right)
     \]
     - \(|D|\) — общее количество документов.
     - \(\text{df}(t)\) — количество документов, содержащих термин \(t\).
     - Добавление 1 в числителе и знаменателе предотвращает нулевые значения и деление на 0.
   - **Смысл**: Уменьшает вес часто встречающихся слов (например, "гондор" во всех документах).

3. **TF-IDF**:
   - **Определение**: Комбинация TF и IDF, дающая вес термина, который отражает его важность в документе и редкость в корпусе.
   - **Формула**:  
     \[
     TF\text{-}IDF(t, d, D) = TF(t, d) \cdot IDF(t, D)
     \]
   - **Смысл**: Высокий TF-IDF означает, что слово важно для документа и редко встречается в других.

4. **Косинусное сходство**:
   - **Определение**: Мера сходства между двумя векторами (запроса и документа) на основе угла между ними.
   - **Формула**:  
     \[
     \text{Cosine Similarity}(A, B) = \frac{A \cdot B}{\|A\| \cdot \|B\|}
     \]
     - \(A \cdot B\) — скалярное произведение векторов.
     - \(\|A\|\), \(\|B\|\) — евклидовы нормы векторов (\(\sqrt{\sum a_i^2}\)).
   - **Смысл**: Значение от 0 до 1, где 1 — полное совпадение, 0 — ортогональность.

---

### Функции в коде

#### 1. `load_documents(data_path)`
- **Назначение**: Загружает текстовые файлы из указанной директории в словарь.
- **Вход**: Путь к папке (`data_path`).
- **Выход**: Словарь `documents`, где ключ — имя файла, значение — содержимое файла.
- **Реализация**: 
  - Использует `os.listdir` для перебора файлов.
  - Фильтрует только `.txt` файлы.
  - Читает файлы с кодировкой UTF-8.
- **Теория**: Это подготовительный этап, обеспечивающий доступ к данным для анализа.

#### 2. `preprocess_text(text)`
- **Назначение**: Преобразует текст в список токенов, готовых для анализа (токенизация, стемминг, удаление стоп-слов).
- **Вход**: Строка текста.
- **Выход**: Список обработанных токенов.
- **Реализация**: 
  - Приводит текст к нижнему регистру.
  - Токенизирует с помощью `word_tokenize` (NLTK).
  - Применяет стемминг через `SnowballStemmer("russian")`.
  - Удаляет стоп-слова (`stopwords.words('russian')`) и фильтрует токены, содержащие буквы (`any(c.isalpha() for c in word)`).
- **Теория**: 
  - Токенизация разбивает текст на слова.
  - Стемминг приводит слова к базовой форме (например, "затрепетали" → "затрепета").
  - Удаление стоп-слов снижает шум, убирая малозначимые слова ("на", "в").

#### 3. `build_tf_matrix_and_doc_freq(documents)`
- **Назначение**: Строит матрицу TF и подсчитывает частоту документов для каждого слова.
- **Вход**: Словарь `documents`.
- **Выход**: 
  - `tf_matrix` — словарь TF-оценок для каждого документа.
  - `vocab` — множество уникальных слов.
  - `doc_freq` — словарь числа документов с каждым словом.
  - `preprocessed_docs` — кэш предобработанных токенов.
- **Реализация**: 
  - Для каждого документа:
    - Вызывает `preprocess_text` для получения токенов.
    - Считает частоту слов (`word_count`).
    - Вычисляет TF как \( \text{count} / \text{total_words} \).
    - Подсчитывает `doc_freq` через уникальные слова (`set(tokens)`).
- **Теория**: 
  - Создаёт основу для TF-IDF, подсчитывая локальную частоту (TF) и глобальную частоту (df).

#### 4. `compute_idf(total_docs, doc_freq, vocab)`
- **Назначение**: Вычисляет IDF для каждого слова в словаре.
- **Вход**: 
  - `total_docs` — общее число документов.
  - `doc_freq` — частота документов для слов.
  - `vocab` — множество слов.
- **Выход**: Словарь `idf` с IDF-значениями.
- **Реализация**: 
  - Для каждого слова:  
    \[
    IDF = \log\left(1 + \frac{\text{total_docs}}{\text{df} + 1}\right)
    \]
  - Логирует случаи, если IDF = 0 (для отладки).
- **Теория**: 
  - IDF выделяет редкие слова, снижая вес частых. Добавление 1 обеспечивает ненулевые значения.

#### 5. `build_tfidf_matrix(tf_matrix, idf)`
- **Назначение**: Создаёт матрицу TF-IDF.
- **Вход**: 
  - `tf_matrix` — матрица TF.
  - `idf` — словарь IDF.
- **Выход**: Словарь `tfidf_matrix`.
- **Реализация**: 
  - Для каждого документа умножает TF на IDF:  
    \[
    TF\text{-}IDF = TF \cdot IDF
    \]
- **Теория**: 
  - Итоговая матрица представляет документы как векторы в пространстве слов.

#### 6. `query_to_tfidf_vector(query, idf, vocab)`
- **Назначение**: Преобразует запрос в TF-IDF вектор.
- **Вход**: 
  - `query` — строка запроса.
  - `idf` — словарь IDF.
  - `vocab` — словарь слов.
- **Выход**: Словарь с TF-IDF значениями для слов запроса.
- **Реализация**: 
  - Токенизирует запрос через `preprocess_text`.
  - Вычисляет TF как \( \text{count} / \text{total_words} \).
  - Умножает TF на IDF для слов из `vocab`.
- **Теория**: 
  - Преобразует запрос в тот же векторный формат, что и документы.

#### 7. `cosine_similarity(vec1, vec2)`
- **Назначение**: Вычисляет косинусное сходство между двумя векторами.
- **Вход**: Два словаря с TF-IDF значениями (`vec1`, `vec2`).
- **Выход**: Число от 0 до 1.
- **Реализация**: 
  - Скалярное произведение: \(\sum (v1 \cdot v2)\).
  - Нормы: \(\sqrt{\sum v1^2}\), \(\sqrt{\sum v2^2}\).
  - Формула:  
    \[
    \text{similarity} = \frac{\text{dot_product}}{\text{norm1} \cdot \text{norm2}}
    \]
  - Возвращает 0, если нормы нулевые.
- **Теория**: 
  - Измеряет угол между векторами, показывая их сходство.

#### 8. `rank_documents(query, tfidf_matrix, idf, vocab)`
- **Назначение**: Ранжирует документы по сходству с запросом.
- **Вход**: 
  - `query` — запрос.
  - `tfidf_matrix` — матрица TF-IDF.
  - `idf`, `vocab` — для создания вектора запроса.
- **Выход**: Список пар (документ, сходство), отсортированный по убыванию.
- **Реализация**: 
  - Создаёт вектор запроса.
  - Вычисляет косинусное сходство для каждого документа.
  - Сортирует по убыванию.
- **Теория**: 
  - Ранжирование основано на близости векторов в пространстве.

#### 9. `simple_count_search(query, preprocessed_docs)`
- **Назначение**: Простой поиск по количеству вхождений слов запроса.
- **Вход**: 
  - `query` — запрос.
  - `preprocessed_docs` — предобработанные токены документов.
- **Выход**: Список пар (документ, количество), отсортированный по убыванию.
- **Реализация**: 
  - Токенизирует запрос.
  - Подсчитывает вхождения каждого слова запроса в документе.
- **Теория**: 
  - Простая альтернатива TF-IDF, игнорирующая редкость слов.

#### 10. `generate_relevant_docs(queries, preprocessed_docs)`
- **Назначение**: Определяет релевантные документы для запросов.
- **Вход**: 
  - `queries` — список запросов.
  - `preprocessed_docs` — токены документов.
- **Выход**: Словарь `relevant_docs`.
- **Реализация**: 
  - Для каждого запроса проверяет, содержат ли документы все его токены.
  - Если нет совпадений, добавляет первый документ.
- **Теория**: 
  - Автоматическая разметка для оценки качества.

#### 11. `evaluate_search(ranked_docs, relevant_docs)`
- **Назначение**: Оценивает качество поиска через метрики Precision, Recall, F1.
- **Вход**: 
  - `ranked_docs` — ранжированный список документов.
  - `relevant_docs` — список релевантных документов.
- **Выход**: Кортеж (Precision, Recall, F1).
- **Реализация**: 
  - Precision: \(\frac{\text{найденные релевантные}}{\text{все найденные}}\).
  - Recall: \(\frac{\text{найденные релевантные}}{\text{все релевантные}}\).
  - F1: \(\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\).
- **Теория**: 
  - Метрики оценивают точность и полноту поиска.

---

### Основной код
- **Назначение**: Управляет процессом: загрузка, подготовка, анализ, визуализация.
- **Реализация**: 
  - Загружает документы.
  - Строит матрицы TF-IDF.
  - Выполняет поиск и оценку для каждого запроса.
  - Строит график времени выполнения.
- **Теория**: 
  - Связывает все этапы в единый процесс.

---

### Итог

- **TF-IDF**: Комбинирует локальную (TF) и глобальную (IDF) значимость слов, позволяя ранжировать документы по релевантности.
- **Косинусное сходство**: Эффективно измеряет близость векторов.
- **NLTK**: Улучшает обработку текста через токенизацию и стемминг.
- **Метрики**: Дают количественную оценку качества поиска.

Если нужно углубиться в какую-то часть (например, вывести конкретные примеры расчётов), дайте знать!